{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800a1e89",
   "metadata": {},
   "source": [
    "# # Step 1: Setup and Configuration\n",
    "Import all necessary libraries and configure input/output paths.\n",
    "The `sample` flag can be set to `True` to run the pipeline on a smaller dataset for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f59271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw XML Input Directory: ./grobid_full_text\n",
      "Final Output Directory: ./database_ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from lxml import etree as ET\n",
    "from uuid import uuid4\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import random\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DATA_DIR = './'\n",
    "RAW_XML_DIR = os.path.join(BASE_DATA_DIR, 'grobid_full_text')\n",
    "TEST_TRAIN_DIR = os.path.join(BASE_DATA_DIR, 'test_train_data')\n",
    "FINAL_OUTPUT_DIR = os.path.join(BASE_DATA_DIR, 'database_ready')\n",
    "    \n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define XML Namespaces for TEI parsing\n",
    "NS = {\n",
    "    'tei': 'http://www.tei-c.org/ns/1.0',\n",
    "    'xml': 'http://www.w3.org/XML/1998/namespace'\n",
    "}\n",
    "print(f\"Raw XML Input Directory: {RAW_XML_DIR}\")\n",
    "print(f\"Final Output Directory: {FINAL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d6d6b",
   "metadata": {},
   "source": [
    "# # Step 2: Helper Functions\n",
    "This section contains all the helper functions used throughout the pipeline for parsing, cleaning, and structuring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- XML Parsing and Cleaning Functions (from 1__preprocess.ipynb & 2__data_cleaning.ipynb) ---\n",
    "\n",
    "def convert_ref_tags(text):\n",
    "    \"\"\"Converts various <ref> tag formats into a standardized one.\"\"\"\n",
    "    # Convert ref tags without a target\n",
    "    if re.search(r'<ref type=\"bibr\">(.*?)</ref>', text):\n",
    "        pattern = r'<ref type=\"bibr\">(.*?)</ref>'\n",
    "        replacement = r'<ref type=\"single\" target=\"#n999\">\\1</ref>'\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    # Standardize targeted ref tags\n",
    "    pattern = r'<ref type=\"bibr\" target=\"#(.*?)\">(.*?)</ref>'\n",
    "    replacement = r'<ref type=\"single\" target=\"#\\1\">\\2</ref>'\n",
    "    return re.sub(pattern, replacement, text)\n",
    "\n",
    "def replace_with_gref(match):\n",
    "    \"\"\"Groups consecutive single reference tags into one group reference tag.\"\"\"\n",
    "    all_refs = match.group(0)\n",
    "    matches = re.findall(r'<ref type=\"single\" target=\"(#[a-z]\\d+)\">(.*?)</ref>', all_refs)\n",
    "    ids = ';'.join([match[0] for match in matches])\n",
    "    content = ' '.join([match[1] for match in matches])\n",
    "    return f'<ref type=\"group\" target=\"{ids}\">{content}</ref>'\n",
    "\n",
    "def process_paragraphs(paragraphs):\n",
    "    \"\"\"Cleans and processes a list of paragraphs.\"\"\"\n",
    "    processed_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        processed_paragraph = convert_ref_tags(paragraph)\n",
    "        # Pattern for consecutive single ref tags\n",
    "        pattern = r'(?:<ref type=\"single\" target=\"#[a-z]\\d+\">[^<]+</ref>\\s*?)+<ref type=\"single\" target=\"#[a-z]\\d+\">([^<]+)</ref>'\n",
    "        clean_paragraph = re.sub(pattern, replace_with_gref, processed_paragraph)\n",
    "        # Remove paragraph tags and other unwanted tags\n",
    "        clean_paragraph = re.sub(r'<p[^>]+>(.*?)</p>', r'\\1', clean_paragraph)\n",
    "        clean_paragraph = re.sub(r'<(?=(?!/))(?!ref)[^>]+>[^<]+<[^>]+>', '', clean_paragraph)\n",
    "        processed_paragraphs.append(clean_paragraph)\n",
    "    return processed_paragraphs\n",
    "\n",
    "def clean_div(div):\n",
    "    \"\"\"Cleans a section (div) by removing unwanted tags and structuring paragraphs.\"\"\"\n",
    "    # Remove non-bibr ref tags but keep content\n",
    "    cleaned_div = re.sub(r'<ref type=\"(?!bibr)[^>]+>([^<]+)</ref>', r'\\1', div)\n",
    "    # Remove p tags around formulas\n",
    "    cleaned_div = re.sub(r'(</p>)?<formula[^>]+>([^<]+)<.*?/formula>(<p>)?', r' \\2 \\3\\1', cleaned_div)\n",
    "    cleaned_div = re.sub(r'<p></p>', '', cleaned_div)\n",
    "    # Extract content within <p> tags\n",
    "    cleaned_div = re.findall(r'<p>(.*?)</p>', cleaned_div)\n",
    "    return cleaned_div\n",
    "\n",
    "# --- Data Structuring Functions (from 3__structure_data.ipynb) ---\n",
    "\n",
    "def concat_auth(authors):\n",
    "    \"\"\"Concatenates author names into a single string.\"\"\"\n",
    "    if not authors:\n",
    "        return 'unknown'\n",
    "    return ' ,'.join([' '.join([v if v is not None else 'unk' for v in auth.values()]) for auth in authors])\n",
    "\n",
    "def get_uuid_of_doc(title, pub_year, authors, abstract, doc_df, lookup_dict):\n",
    "    \"\"\"\n",
    "    Checks if a document already exists in the DataFrame.\n",
    "    If yes, returns its UUID. If no, adds it and returns the new UUID.\n",
    "    Uses a lookup dictionary for efficiency.\n",
    "    \"\"\"\n",
    "    title = title if title else 'unknown'\n",
    "    pub_year = pub_year if pub_year else 'unknown'\n",
    "    \n",
    "    lookup_key = (title.lower(), str(pub_year))\n",
    "    \n",
    "    if lookup_key in lookup_dict:\n",
    "        return lookup_dict[lookup_key]\n",
    "    else:\n",
    "        new_id = str(uuid4())\n",
    "        new_row = {'id': new_id, 'title': title, 'pub_year': pub_year, 'authors': authors, 'abstract': abstract}\n",
    "        doc_df.loc[len(doc_df)] = new_row\n",
    "        lookup_dict[lookup_key] = new_id\n",
    "        return new_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2ff23",
   "metadata": {},
   "source": [
    "# # Step 3: Parse Raw XML Data\n",
    "Iterates through the raw XML files, parses them using lxml, and extracts relevant information into a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb1befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XML parsing...\n",
      "Processing file 1/1: 2025.findings-acl.1259.pdf.tei.xml\\r\\nParsing complete. Successfully parsed 1 files. Encountered 0 errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/97/3t8jpk6n7psfw3dq1jrvdmz80000gn/T/ipykernel_30740/1380459502.py:41: FutureWarning: Truth-testing of elements was a source of confusion and will always return True in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not ref_title:\n",
      "/var/folders/97/3t8jpk6n7psfw3dq1jrvdmz80000gn/T/ipykernel_30740/1380459502.py:43: FutureWarning: Truth-testing of elements was a source of confusion and will always return True in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not ref_title:\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting XML parsing...\")\n",
    "parsed_documents = []\n",
    "error_files = []\n",
    "xml_files = [f for f in os.listdir(RAW_XML_DIR) if f.endswith('.xml')]\n",
    "\n",
    "for i, filename in enumerate(xml_files):\n",
    "    xml_file_path = os.path.join(RAW_XML_DIR, filename)\n",
    "    print(f\"Processing file {i+1}/{len(xml_files)}: {filename}\", end='\\\\r')\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract metadata\n",
    "        title = root.find(\".//tei:teiHeader//tei:title[@type='main']\", NS)\n",
    "        abstract = root.find('.//tei:teiHeader//tei:abstract//tei:p', NS)\n",
    "        text_lang = root.find('tei:text', NS).get(f'{{{NS[\"xml\"]}}}lang')\n",
    "        \n",
    "        authors = []\n",
    "        for author in root.findall('.//tei:teiHeader//tei:author/tei:persName', NS):\n",
    "            first = author.find('tei:forename', NS)\n",
    "            last = author.find('tei:surname', NS)\n",
    "            authors.append({'first_name': first.text if first is not None else None, \n",
    "                            'last_name': last.text if last is not None else None})\n",
    "\n",
    "        doc_pub_year_str = filename.split('.')[0]\n",
    "        doc_pub_year = int(doc_pub_year_str) if doc_pub_year_str.isnumeric() else None\n",
    "\n",
    "        # Extract sections and paragraphs\n",
    "        sections = []\n",
    "        for div in root.findall('.//tei:body/tei:div', NS):\n",
    "            section_name = div.find('.//tei:head', NS)\n",
    "            paragraphs_raw = clean_div(ET.tostring(div, encoding='unicode', method='xml'))\n",
    "            processed_paragraphs = process_paragraphs(paragraphs_raw)\n",
    "            sections.append({'section_name': section_name.text if section_name is not None else None, \n",
    "                             'paragraphs': processed_paragraphs})\n",
    "        # Extract references\n",
    "        references = []\n",
    "        for bibl in root.findall('.//tei:listBibl/tei:biblStruct', NS):\n",
    "            ref_id = bibl.get(f'{{{NS[\"xml\"]}}}id', \"Unknown ID\")\n",
    "            ref_title = bibl.find(\".//tei:title[@type='main']\", NS)\n",
    "            if not ref_title:\n",
    "                ref_title = bibl.find(\".//tei:title[@level='a']\", NS)\n",
    "            if not ref_title:\n",
    "                ref_title = bibl.find(\".//tei:title\", NS)\n",
    "            pub_date = bibl.find('.//tei:date[@type=\"published\"]', NS)\n",
    "            \n",
    "            ref_authors = []\n",
    "            for author in bibl.findall('.//tei:author/tei:persName', NS):\n",
    "                first = author.find('tei:forename', NS)\n",
    "                last = author.find('tei:surname', NS)\n",
    "                ref_authors.append({'first_name': first.text if first is not None else None, \n",
    "                                    'last_name': last.text if last is not None else None})\n",
    "            \n",
    "            pub_year = None\n",
    "            if pub_date is not None and pub_date.get('when'):\n",
    "                match = re.search(r'[1-2][0,9]\\d{2}', pub_date.get('when'))\n",
    "                if match:\n",
    "                    pub_year = int(match.group(0))\n",
    "            \n",
    "            references.append({\n",
    "                'id': ref_id,\n",
    "                'title': ref_title.text if ref_title is not None else None,\n",
    "                'authors': ref_authors,\n",
    "                'pub_year': pub_year,\n",
    "            })\n",
    "\n",
    "        parsed_documents.append({\n",
    "            'filename': filename,\n",
    "            'title': title.text if title is not None else None,\n",
    "            'authors': authors,\n",
    "            'pub_year': doc_pub_year,\n",
    "            'lang': text_lang,\n",
    "            'abstract': abstract.text if abstract is not None else None,\n",
    "            'sections': sections,\n",
    "            'references': references\n",
    "        })\n",
    "\n",
    "    except ET.ParseError:\n",
    "        error_files.append(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {filename}: {e}\")\n",
    "        error_files.append(filename)\n",
    "\n",
    "print(f\"\\\\nParsing complete. Successfully parsed {len(parsed_documents)} files. Encountered {len(error_files)} errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28be6b5",
   "metadata": {},
   "source": [
    "# # Step 4: Filter and Clean Parsed Data\n",
    "This step applies quality filters to the parsed data. It removes documents that are likely to be of low quality or irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35772d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and cleaning documents...\n",
      "Cleaning complete. 1 documents remain after filtering.\n",
      "Total paragraphs in cleaned documents: 68\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering and cleaning documents...\")\n",
    "cleaned_documents = []\n",
    "total_paragraphs_count = 0\n",
    "\n",
    "for doc in parsed_documents:\n",
    "    # --- Apply Filters ---\n",
    "    # Skip documents with faulty metadata or insufficient size\n",
    "    if (doc['title'] is None or\n",
    "        len(doc['authors']) < 1 or\n",
    "        any(auth.get('last_name') is None for auth in doc['authors']) or\n",
    "        doc['pub_year'] is None or not (1950 <= doc['pub_year'] <= 2025) or\n",
    "        len(doc['references']) < 5 or\n",
    "        len(doc['sections']) < 3 or\n",
    "        doc['lang'] != 'en'):\n",
    "        continue\n",
    "    \n",
    "    # Filter out sections that contain no reference tags\n",
    "    filtered_sections = []\n",
    "    for section in doc.get('sections', []):\n",
    "        pars_joined = ''.join(section.get('paragraphs', []))\n",
    "        if re.search(r'<ref.*?</ref>', pars_joined):\n",
    "            filtered_sections.append(section)\n",
    "            total_paragraphs_count += len(section.get('paragraphs', []))\n",
    "    \n",
    "    doc['sections'] = filtered_sections\n",
    "\n",
    "    # Re-check for insufficient length after filtering sections\n",
    "    if len(doc['references']) < 5 or len(doc['sections']) < 3:\n",
    "        continue\n",
    "        \n",
    "    cleaned_documents.append(doc)\n",
    "\n",
    "print(f\"Cleaning complete. {len(cleaned_documents)} documents remain after filtering.\")\n",
    "print(f\"Total paragraphs in cleaned documents: {total_paragraphs_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e88b3e",
   "metadata": {},
   "source": [
    "# # Step 5: Structure Data into Relational Format\n",
    "The cleaned data is now transformed into a relational structure using Pandas DataFrames, with unique IDs for documents, paragraphs, and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc834e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structuring data into relational tables...\n",
      "Structuring complete.\n",
      "Created 37 document entries.\n",
      "Created 68 paragraph entries.\n",
      "Created 58 reference entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Structuring data into relational tables...\")\n",
    "\n",
    "# Define DataFrame schemas\n",
    "citation_columns = ['id', 'marker_location', 'type', 'cited_doc_id', 'paragraph_id']\n",
    "paragraph_columns = ['id', 'text', 'section_id']\n",
    "section_columns = ['id', 'section_title', 'document_id']\n",
    "doc_columns = ['id', 'title', 'pub_year', 'authors', 'abstract']\n",
    "\n",
    "citation_df = pd.DataFrame(columns=citation_columns)\n",
    "parargraph_df = pd.DataFrame(columns=paragraph_columns)\n",
    "section_df = pd.DataFrame(columns=section_columns)\n",
    "document_df = pd.DataFrame(columns=doc_columns)\n",
    "\n",
    "unknown_doc_id = str(uuid4())\n",
    "document_df.loc[0] = [unknown_doc_id, 'unknown', 'unknown', 'unknown', 'unknown']\n",
    "\n",
    "# Initialize a lookup dictionary for faster document checks\n",
    "doc_lookup_dict = {}\n",
    "\n",
    "# Initialize MWE Tokenizer for locating reference tags\n",
    "tokenizer = MWETokenizer(separator='')\n",
    "tokenizer.add_mwe(('[', 'REF', ']'))\n",
    "\n",
    "for doc_data in cleaned_documents:\n",
    "    # Get/create UUID for the main document\n",
    "    doc_id = get_uuid_of_doc(doc_data['title'], doc_data['pub_year'], concat_auth(doc_data['authors']), doc_data['abstract'], document_df, doc_lookup_dict)\n",
    "    \n",
    "    # Create a dictionary mapping reference IDs (e.g., 'b23') to cited document UUIDs\n",
    "    ref_id_targets = {}\n",
    "    for ref in doc_data['references']:\n",
    "        cited_doc_uuid = get_uuid_of_doc(ref['title'], ref['pub_year'], concat_auth(ref['authors']), None, document_df, doc_lookup_dict)\n",
    "        ref_id_targets[ref['id']] = cited_doc_uuid\n",
    "\n",
    "    # Process paragraphs and references\n",
    "    for section in doc_data['sections']:\n",
    "        sec_id = str(uuid4())\n",
    "        section_df.loc[len(section_df)] = [sec_id, section['section_name'], doc_id]\n",
    "        for paragraph_text in section['paragraphs']:\n",
    "            par_id = str(uuid4())\n",
    "            \n",
    "            # Find all original <ref> tags and replace them with a placeholder\n",
    "            references_xml = re.findall(r'(<ref.*?</ref>)', paragraph_text)\n",
    "            cleaned_text = re.sub(r'(<ref.*?</ref>)', ' [REF] ', paragraph_text)\n",
    "            cleaned_text = re.sub(r';', ',', cleaned_text) # Normalize semicolons\n",
    "            \n",
    "            # Tokenize and find the locations of the placeholders\n",
    "            tokenized_text = tokenizer.tokenize(cleaned_text.split())\n",
    "            marker_locations = [i for i, token in enumerate(tokenized_text) if '[REF]' in token]\n",
    "\n",
    "            # If counts match, replace placeholders with cleaned ref tags\n",
    "            if len(marker_locations) == len(references_xml):\n",
    "                for i, loc in enumerate(marker_locations):\n",
    "                    tokenized_text[loc] = re.sub(r' target=\".*?\"', '', references_xml[i])\n",
    "            \n",
    "            # Add paragraph to DataFrame\n",
    "            parargraph_df.loc[len(parargraph_df)] = [par_id, ';'.join(tokenized_text), sec_id]\n",
    "\n",
    "            # Add references to DataFrame\n",
    "            for i, ref_xml in enumerate(references_xml):\n",
    "                match = re.search(r'type=\"(.*?)\" target=\"(.*?)\"', ref_xml)\n",
    "                if match:\n",
    "                    ref_type = match.group(1)\n",
    "                    target_ids = [m.replace('#', '') for m in match.group(2).split(';')]\n",
    "                    cited_doc_uuids = [ref_id_targets.get(ref_id, unknown_doc_id) for ref_id in target_ids]\n",
    "                    \n",
    "                    new_ref = {\n",
    "                        'id': str(uuid4()),\n",
    "                        'marker_location': marker_locations[i],\n",
    "                        'type': ref_type,\n",
    "                        'cited_doc_id': ';'.join(cited_doc_uuids),\n",
    "                        'paragraph_id': par_id\n",
    "                    }\n",
    "                    citation_df.loc[len(citation_df)] = new_ref\n",
    "\n",
    "print(\"Structuring complete.\")\n",
    "print(f\"Created {len(document_df)} document entries.\")\n",
    "print(f\"Created {len(parargraph_df)} paragraph entries.\")\n",
    "print(f\"Created {len(citation_df)} reference entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf31421",
   "metadata": {},
   "source": [
    "# # Step 7: Normalize Tables for Database Import\n",
    "This final step creates a dedicated linking table for cited documents to ensure the data is in a normalized form, which is better for relational databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac138df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing tables...\n",
      "Created linking table with 89 entries.\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing tables...\")\n",
    "\n",
    "cited_doc_df = pd.DataFrame(columns=['citation_id', 'document_id'])\n",
    "rows_to_add = []\n",
    "\n",
    "for _, row in citation_df.iterrows():\n",
    "    for cited_doc_id in row['cited_doc_id'].split(';'):\n",
    "        rows_to_add.append({'citation_id': row['id'], 'document_id': cited_doc_id})\n",
    "\n",
    "if rows_to_add:\n",
    "    cited_doc_df = pd.DataFrame(rows_to_add)\n",
    "\n",
    "cited_doc_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop the now-redundant column from the original ref_df\n",
    "citation_df.drop(columns=['cited_doc_id'], inplace=True)\n",
    "\n",
    "print(f\"Created linking table with {len(cited_doc_df)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e408f",
   "metadata": {},
   "source": [
    "# # Step 8: Create Annotation Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51eb28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame(columns=['user_id', 'citation_id'])\n",
    "user_ids = {\n",
    "    'max':'17a7019f-4eb1-40fb-8c66-e2953af5f4be',\n",
    "    'erika':'907f4be7-ac0e-480c-8e51-a99f52011653',\n",
    "    }\n",
    "\n",
    "user_id_list = user_ids.values()\n",
    "\n",
    "for par_id in parargraph_df['id'].to_list():\n",
    "    par_cit_ids = citation_df[citation_df['paragraph_id'] == par_id]['id'].to_list()\n",
    "    if not par_cit_ids:continue\n",
    "    ref_id = random.sample(par_cit_ids,1)[0]\n",
    "    annotations_df.loc[len(annotations_df)] = [user_ids['max'], ref_id]\n",
    "    annotations_df.loc[len(annotations_df)] = [user_ids['erika'], ref_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9875e67",
   "metadata": {},
   "source": [
    "# # Step 9: Save Final Output\n",
    "The processed and structured DataFrames are saved to CSV files in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db584329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final CSV files to ./database_ready...\n",
      "Pipeline finished successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Saving final CSV files to {FINAL_OUTPUT_DIR}...\")\n",
    "\n",
    "citation_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_citations.csv'), index=False)\n",
    "parargraph_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_paragrpahs.csv'), index=False)\n",
    "section_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_sections.csv'), index=False)\n",
    "document_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_documents.csv'), index=False)\n",
    "cited_doc_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_cited_documents.csv'), index=False)\n",
    "annotations_df.to_csv(os.path.join(FINAL_OUTPUT_DIR, 'db_annotations.csv'), index=False)\n",
    "\n",
    "print(\"Pipeline finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
